{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: A First Model\n",
    "\n",
    "After getting a basic understanding of the data in the previous tutorial, we are ready to build a first AI model.\n",
    "In this notebook, we build and evaluate a simple model for identifying worm sections in nodule images.\n",
    "\n",
    "By the end of this notebook you will have created a first submission that you can submit to put your team on the leaderboard!\n",
    "\n",
    "**NOTE: This notebook will only work with a GPU instance. Make sure that you selected *ml.g4dn.xlarge* as instance type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking the right approach\n",
    "\n",
    "The first challenge we have to address is how to even tackle the problem. What could be a good approach / algorithm? How are other people doing it?\n",
    "[PapersWithCode](https://paperswithcode.com/sota) is an amazing resource for exactly these questions. It lists typical problem classes together with the current state-of-the-art and links to popular frameworks.\n",
    "For our problem, [Object Detection](https://paperswithcode.com/task/object-detection) is the right category. If you follow the link you'll find a list of popular frameworks.\n",
    "\n",
    "[MMDetection](https://github.com/open-mmlab/mmdetection) is one of the most widely used and the one we picked for this tutorial. Not only does it provide a great [tutorial](https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb), but also offers a variety of [different architectures and settings](https://github.com/open-mmlab/mmdetection#overview-of-benchmark-and-model-zoo) that you can play around with.\n",
    "\n",
    "In this notebook, we will adapt the MMdetection [tutorial](https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb) to our problem and run it on a toy dataset.\n",
    "\n",
    "For those familiar with deep learning architectures, MMDetection can handle data loading, preperation, model building, and training by altering a few common settings in the configuration file. Data preperation also includes various data augmentation techinques like rotation or flipping. The majority of the most common CV arcitectures are already build and available via configuration.\n",
    "\n",
    "### *Further reading:*\n",
    "Going into details of object detection is beyond the scope of this tutorial. If you'd like to know more, here are two recommendations to get started:\n",
    "- [An introduction to R-CNNs and object detection in general](https://towardsdatascience.com/deep-dive-into-the-computer-vision-world-part-2-7a24efdb1a14)\n",
    "- [An graphic explanation how convolutional neural networks work](https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "As in the other notebooks, we start by importing the relevant libraries and global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Used for plotting\n",
    "import mmcv  # Object detection framework\n",
    "import os  # Interaction with the file system\n",
    "import pandas as pd  # Home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import sys  # Python system library needed to load custom functions\n",
    "\n",
    "from matplotlib.patches import Rectangle  # Allows drawing the bounding boxes of the worm sections\n",
    "from mmcv import Config  # Loading and accessing MMDetection configuration files\n",
    "from mmdet.apis import inference_detector, init_detector, train_detector, set_random_seed  # Part of the MMDetection framework\n",
    "from mmdet.datasets import build_dataset  # Part of the MMDetection framework\n",
    "from mmdet.models import build_detector  # Part of the MMDetection framework\n",
    "\n",
    "from PIL import Image  # For loading image files\n",
    "from tqdm import tqdm  # for timing a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')  # Add the source directory to the PYTHONPATH. This allows to import local functions and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import OnchoDataset\n",
    "from detection_util import create_predictions\n",
    "from gdsc_score import get_leaderboard_score\n",
    "from gdsc_util import download_directory, download_file, load_sections_df, set_up_logging, PROJECT_DIR\n",
    "from PredictionEvaluator import PredictionEvaluator\n",
    "\n",
    "set_up_logging()  # Sets up logging to console and the .log file\n",
    "data_folder = str(PROJECT_DIR / 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Training Dataset\n",
    "\n",
    "Whenever you start working with a new framework you can expect issues with the setup.\n",
    "To ensure that we can quickly try things out we create a small subset of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65687"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_df = load_sections_df(f'{data_folder}/gdsc_train.csv')\n",
    "len(section_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our complete dataset has more than 65.000 rows. Let's create two small dummy sets with 100 and 50 entries only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = section_df[:100]\n",
    "dummy_test = section_df[100:150]\n",
    "dummy_train.to_csv(f'{data_folder}/dummy_train.csv', sep=';')\n",
    "dummy_test.to_csv(f'{data_folder}/dummy_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the reduced sets under *../data*. Verify that you can find and open the files there. \n",
    "To make sure that everything works, let's load the files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>study</th>\n",
       "      <th>staining</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_D@2317-5556-2836-6232</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>Study_1</td>\n",
       "      <td>D</td>\n",
       "      <td>2317</td>\n",
       "      <td>2836</td>\n",
       "      <td>5556</td>\n",
       "      <td>6232</td>\n",
       "      <td>8192</td>\n",
       "      <td>7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D@2407-6156-2952-6789</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>Study_1</td>\n",
       "      <td>D</td>\n",
       "      <td>2407</td>\n",
       "      <td>2952</td>\n",
       "      <td>6156</td>\n",
       "      <td>6789</td>\n",
       "      <td>8192</td>\n",
       "      <td>7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D@2483-5836-4279-7287</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>Study_1</td>\n",
       "      <td>D</td>\n",
       "      <td>2483</td>\n",
       "      <td>4279</td>\n",
       "      <td>5836</td>\n",
       "      <td>7287</td>\n",
       "      <td>8192</td>\n",
       "      <td>7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D@2546-1211-4374-2558</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>Study_1</td>\n",
       "      <td>D</td>\n",
       "      <td>2546</td>\n",
       "      <td>4374</td>\n",
       "      <td>1211</td>\n",
       "      <td>2558</td>\n",
       "      <td>8192</td>\n",
       "      <td>7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D@2695-5530-3238-6179</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>Study_1</td>\n",
       "      <td>D</td>\n",
       "      <td>2695</td>\n",
       "      <td>3238</td>\n",
       "      <td>5530</td>\n",
       "      <td>6179</td>\n",
       "      <td>8192</td>\n",
       "      <td>7380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name    study staining  xmin  xmax  ymin  ymax  \\\n",
       "section_id                                                                    \n",
       "1_D@2317-5556-2836-6232   1_D.jpg  Study_1        D  2317  2836  5556  6232   \n",
       "1_D@2407-6156-2952-6789   1_D.jpg  Study_1        D  2407  2952  6156  6789   \n",
       "1_D@2483-5836-4279-7287   1_D.jpg  Study_1        D  2483  4279  5836  7287   \n",
       "1_D@2546-1211-4374-2558   1_D.jpg  Study_1        D  2546  4374  1211  2558   \n",
       "1_D@2695-5530-3238-6179   1_D.jpg  Study_1        D  2695  3238  5530  6179   \n",
       "\n",
       "                         height  width  \n",
       "section_id                              \n",
       "1_D@2317-5556-2836-6232    8192   7380  \n",
       "1_D@2407-6156-2952-6789    8192   7380  \n",
       "1_D@2483-5836-4279-7287    8192   7380  \n",
       "1_D@2546-1211-4374-2558    8192   7380  \n",
       "1_D@2695-5530-3238-6179    8192   7380  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_train = load_sections_df(f'{data_folder}/dummy_train.csv')\n",
    "dummy_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a MMDetection Configuration File\n",
    "\n",
    "MMDetection relies on extensive configuration files. The usual process is to adapt an already existing configuration file and apply transfer learning.\n",
    "To do this, we first need to download the configuration and weights files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/checkpoints’: File exists\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease                \u001b[0m\u001b[33m\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu bionic-backports InRelease              \u001b[0m\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease              \u001b[0m\u001b[33m\n",
      "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Err:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Reading package lists... Done0m0m\u001b[33m\u001b[33m\n",
      "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is not signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 84 not upgraded.\n",
      "--2022-05-30 14:10:35--  https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.252.96.28\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.252.96.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a new folder in the data folder\n",
    "!mkdir ../data/checkpoints \n",
    "# Install wget, a program for downloading files from the internet\n",
    "!apt update\n",
    "!apt install wget\n",
    "# Download the config and weights file\n",
    "!wget -c https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth -O ../data/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is part of the mmdetection package that is already installed on this image. Hence we can load our test configuration via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('/mmdetection/configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration is a dictionary-like object that stores our configuration settings. Is has [a lot](https://mmdetection.readthedocs.io/en/latest/tutorials/config.html) of settings and with the right options, you can run the [really advanced models](https://github.com/open-mmlab/mmdetection/blob/master/docs/en/model_zoo.md).\n",
    "Below we modify some of those settings to point the model to our train and test data.\n",
    "\n",
    "We start by defining the structure and location of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'OnchoDataset' #this is a custom data loader script we created for the GDSC you can view it in src/Dataset.py\n",
    "cfg.data_root = data_folder\n",
    "\n",
    "cfg.data.train.type = 'OnchoDataset'\n",
    "cfg.data.train.data_root = data_folder      # path to the folder data\n",
    "cfg.data.train.img_prefix = 'jpgs/'         # path from data_root to the images folder\n",
    "cfg.data.train.ann_file = 'dummy_train.csv' # the file containing the train data labels\n",
    "\n",
    "cfg.data.test.type = 'OnchoDataset'\n",
    "cfg.data.test.data_root = data_folder\n",
    "cfg.data.test.img_prefix = 'jpgs/'\n",
    "cfg.data.test.ann_file = 'dummy_test.csv'\n",
    "\n",
    "cfg.data.val.type = 'OnchoDataset'          # We will not use a separate validation data set in this tutorial, but we need to specify the values to overwrite the COCO defaults.\n",
    "cfg.data.val.data_root = data_folder\n",
    "cfg.data.val.img_prefix = 'jpgs/'\n",
    "cfg.data.val.ann_file = 'dummy_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify where to save the results and where the weights we will use were downloaded to. We also set the number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can still use the pre-trained Mask RCNN model though we do not need to use the mask branch\n",
    "cfg.load_from = f'{data_folder}/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = f'{data_folder}/tutorial_exps/'\n",
    "# Ensure work_dir exists\n",
    "mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.data.samples_per_gpu = 3 # These numbers will change depending on the size of your model and GPU.\n",
    "cfg.data.workers_per_gpu = 1 # These values are what we have found to be best for this model and GPU\n",
    "\n",
    "cfg.device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are the model settings. One thing we have to change is the number of classes to predict. Since we only have one class (worm section), we need to set this to one.\n",
    "We also change the learning rate since we only have one GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify number of classes of the model in box head\n",
    "cfg.model.roi_head.bbox_head.num_classes = 1  # a worm section is the only object we are detecting\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU and multiply by the number of GPU workers.\n",
    "cfg.optimizer.lr = 0.02 / 8 * cfg.data.workers_per_gpu\n",
    "cfg.lr_config.warmup = None\n",
    "cfg.log_config.interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify how and when to evaluate the model and how long to train the model. We will use the [mAP](https://blog.paperspace.com/mean-average-precision/#:~:text=To%20evaluate%20object%20detection%20models,model%20is%20in%20its%20detections.) metric to evaluate how the training is going.\n",
    "\n",
    "For our test run, we will only train for three epochs. For a real training run you'll want to set this number higher.\n",
    "After each epoch, the current weights will be saved in the *work_dir* folder we set above. Also the model will be evaluated on the test data set.\n",
    "This allows us to check if we should continue training or stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the evaluation metric since we use customized dataset.\n",
    "cfg.evaluation.metric = 'mAP'\n",
    "# We can set the evaluation interval to reduce the evaluation times\n",
    "cfg.evaluation.interval = 1\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 1\n",
    "# How long do we want to train\n",
    "cfg.runner.max_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the first Model\n",
    "\n",
    "We're now ready to train a first simple model! The first step is to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 14:10:36,785 - Dataset - INFO - Building Dataset\n",
      "/mmdetection/mmdet/datasets/custom.py:180: UserWarning: CustomDataset does not support filtering empty gt images.\n",
      "  'CustomDataset does not support filtering empty gt images.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       " OnchoDataset Train dataset with number of images 3, and instance counts: \n",
       " +-------------+-------+----------+-------+----------+-------+----------+-------+----------+-------+\n",
       " | category    | count | category | count | category | count | category | count | category | count |\n",
       " +-------------+-------+----------+-------+----------+-------+----------+-------+----------+-------+\n",
       " |             |       |          |       |          |       |          |       |          |       |\n",
       " | 0 [section] | 100   |          |       |          |       |          |       |          |       |\n",
       " +-------------+-------+----------+-------+----------+-------+----------+-------+----------+-------+]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains exactly 100 worm sections, one for each row on the *dummy_train* DataFrame. We also get a warning about filtering empty ground truth images which we can ignore since we don't have those.\n",
    "\n",
    "Next, we initialize the model according to the configuration. *train_cfg* determins on which data to train the model, *test_cfg* on which data to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "model.CLASSES = datasets[0].CLASSES  # Add an attribute for visualization convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can train the model. The log output helps us understand how well the model is doing. \n",
    "Since we set the *evaluation.interval* to *1*, the model will be evaluated after every epoch. In general you want to continue training as long as the *mAP* score increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 14:10:39,192 - Dataset - INFO - Building Dataset\n",
      "2022-05-30 14:10:39,205 - mmdet - INFO - load checkpoint from local path: /root/data/AmazonSageMaker-gdsc-tutorials/data/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth\n",
      "2022-05-30 14:10:39,707 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for roi_head.bbox_head.fc_cls.weight: copying a param with shape torch.Size([81, 1024]) from checkpoint, the shape in current model is torch.Size([2, 1024]).\n",
      "size mismatch for roi_head.bbox_head.fc_cls.bias: copying a param with shape torch.Size([81]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "size mismatch for roi_head.bbox_head.fc_reg.weight: copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape in current model is torch.Size([4, 1024]).\n",
      "size mismatch for roi_head.bbox_head.fc_reg.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([4]).\n",
      "unexpected key in source state_dict: roi_head.mask_head.convs.0.conv.weight, roi_head.mask_head.convs.0.conv.bias, roi_head.mask_head.convs.1.conv.weight, roi_head.mask_head.convs.1.conv.bias, roi_head.mask_head.convs.2.conv.weight, roi_head.mask_head.convs.2.conv.bias, roi_head.mask_head.convs.3.conv.weight, roi_head.mask_head.convs.3.conv.bias, roi_head.mask_head.upsample.weight, roi_head.mask_head.upsample.bias, roi_head.mask_head.conv_logits.weight, roi_head.mask_head.conv_logits.bias\n",
      "\n",
      "2022-05-30 14:10:39,713 - mmdet - INFO - Start running, host: root@gdsc5-smstudio-cust-ml-g4dn-xlarge-8e4f662689f9518ad1d6dbca9f90, work_dir: /root/data/AmazonSageMaker-gdsc-tutorials/data/tutorial_exps\n",
      "2022-05-30 14:10:39,714 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-05-30 14:10:39,715 - mmdet - INFO - workflow: [('train', 1)], max: 3 epochs\n",
      "2022-05-30 14:10:39,715 - mmdet - INFO - Checkpoints will be saved to /root/data/AmazonSageMaker-gdsc-tutorials/data/tutorial_exps by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-30 14:10:41.937 gdsc5-smstudio-cust-ml-g4dn-xlarge-8e4f662689f9518ad1d6dbca9f90:1295 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-05-30 14:10:41.973 gdsc5-smstudio-cust-ml-g4dn-xlarge-8e4f662689f9518ad1d6dbca9f90:1295 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 14:10:48,008 - mmdet - INFO - Saving checkpoint at 1 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 3/3, 0.9 task/s, elapsed: 3s, ETA:     0s\n",
      "---------------iou_thr: 0.5---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 14:10:55,763 - mmdet - INFO - \n",
      "+---------+-----+------+--------+-------+\n",
      "| class   | gts | dets | recall | ap    |\n",
      "+---------+-----+------+--------+-------+\n",
      "| section | 50  | 300  | 0.580  | 0.132 |\n",
      "+---------+-----+------+--------+-------+\n",
      "| mAP     |     |      |        | 0.132 |\n",
      "+---------+-----+------+--------+-------+\n",
      "2022-05-30 14:10:55,765 - mmdet - INFO - Epoch(val) [1][3]\tAP50: 0.1320, mAP: 0.1320\n",
      "2022-05-30 14:11:01,690 - mmdet - INFO - Saving checkpoint at 2 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 3/3, 0.9 task/s, elapsed: 3s, ETA:     0s\n",
      "---------------iou_thr: 0.5---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 14:11:09,386 - mmdet - INFO - \n",
      "+---------+-----+------+--------+-------+\n",
      "| class   | gts | dets | recall | ap    |\n",
      "+---------+-----+------+--------+-------+\n",
      "| section | 50  | 300  | 0.760  | 0.250 |\n",
      "+---------+-----+------+--------+-------+\n",
      "| mAP     |     |      |        | 0.250 |\n",
      "+---------+-----+------+--------+-------+\n",
      "2022-05-30 14:11:09,388 - mmdet - INFO - Epoch(val) [2][3]\tAP50: 0.2500, mAP: 0.2502\n",
      "2022-05-30 14:11:15,307 - mmdet - INFO - Saving checkpoint at 3 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 3/3, 0.9 task/s, elapsed: 3s, ETA:     0s\n",
      "---------------iou_thr: 0.5---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 14:11:23,281 - mmdet - INFO - \n",
      "+---------+-----+------+--------+-------+\n",
      "| class   | gts | dets | recall | ap    |\n",
      "+---------+-----+------+--------+-------+\n",
      "| section | 50  | 300  | 0.820  | 0.373 |\n",
      "+---------+-----+------+--------+-------+\n",
      "| mAP     |     |      |        | 0.373 |\n",
      "+---------+-----+------+--------+-------+\n",
      "2022-05-30 14:11:23,285 - mmdet - INFO - Epoch(val) [3][3]\tAP50: 0.3730, mAP: 0.3725\n"
     ]
    }
   ],
   "source": [
    "train_detector(model, datasets, cfg, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log starts with a warning about a size mismatch that we can ignore. It is due to the fact that the weights we load belong to a model that was trained on the COCO dataset which has 80 different classes whereas we only have one class to detect.\n",
    "\n",
    "Since the *mAP* keep increasing it would make sense to continue training the model for longer. \n",
    "But before we do that let's make sure that the rest of the process works.\n",
    "\n",
    "**Note**: \n",
    "- **If you get an error, try rerunning the notebook starting with the loading of the config.** \n",
    "- **If you get the error *RuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 5-dimensional input of size [1, 3, 3, 928, 768] instead* change the instance to one with a GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model\n",
    "\n",
    "After training the model we can now apply it on a new image and look at the predictions. \n",
    "For this, we first load the image and initalize the model from a stored checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = '1_D.jpg'\n",
    "img =  mmcv.imread(f'{data_folder}/jpgs/{example_image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /root/data/AmazonSageMaker-gdsc-tutorials/data/tutorial_exps/epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "checkpoint = f'{cfg.work_dir}epoch_3.pth' # Select one of the model checkpoints to load in\n",
    "model = init_detector(cfg, checkpoint, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmdetection/mmdet/datasets/utils.py:70: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
      "  'data pipeline in your config file.', UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.0032205e+03, 3.9050674e+03, 4.4722310e+03, 4.4629087e+03,\n",
       "        5.7059830e-01],\n",
       "       [4.7793750e+03, 4.9904180e+03, 5.0820996e+03, 5.6726792e+03,\n",
       "        5.6033736e-01]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections = inference_detector(model, img)\n",
    "print(len(detections))  \n",
    "detections[0][:2]  # Show the first two entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*detections* is a list of all the detected worm sections. Every detection is a 5-tuple of the form *(xmin, ymin, xmax, ymax, score)*, i.e. the location of the bounding box followed by the confidence.\n",
    "\n",
    "**Exercise:**\n",
    "- Plot the predicted worm boxes. You may use the code provided in tutorial 2.\n",
    "- Load the model weights of a different checkpoint, e.g. after the first epoch. What are the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "To get an idea how well we're doing we can run the same evaluation function that will be used for the leaderboard on our example file.\n",
    "For this we first need to convert to model output into the format that is readable by our scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>detection_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_D.jpg@4003-4472-3905-4462</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>4003</td>\n",
       "      <td>3905</td>\n",
       "      <td>4472</td>\n",
       "      <td>4462</td>\n",
       "      <td>0.570598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D.jpg@4779-5082-4990-5672</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>4779</td>\n",
       "      <td>4990</td>\n",
       "      <td>5082</td>\n",
       "      <td>5672</td>\n",
       "      <td>0.560337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D.jpg@2930-3302-7185-7667</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>2930</td>\n",
       "      <td>7185</td>\n",
       "      <td>3302</td>\n",
       "      <td>7667</td>\n",
       "      <td>0.536524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D.jpg@5344-5884-2749-3272</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>5344</td>\n",
       "      <td>2749</td>\n",
       "      <td>5884</td>\n",
       "      <td>3272</td>\n",
       "      <td>0.533923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_D.jpg@4516-4842-1575-1984</th>\n",
       "      <td>1_D.jpg</td>\n",
       "      <td>4516</td>\n",
       "      <td>1575</td>\n",
       "      <td>4842</td>\n",
       "      <td>1984</td>\n",
       "      <td>0.529677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file_name  xmin  ymin  xmax  ymax  detection_score\n",
       "section_id                                                                    \n",
       "1_D.jpg@4003-4472-3905-4462   1_D.jpg  4003  3905  4472  4462         0.570598\n",
       "1_D.jpg@4779-5082-4990-5672   1_D.jpg  4779  4990  5082  5672         0.560337\n",
       "1_D.jpg@2930-3302-7185-7667   1_D.jpg  2930  7185  3302  7667         0.536524\n",
       "1_D.jpg@5344-5884-2749-3272   1_D.jpg  5344  2749  5884  3272         0.533923\n",
       "1_D.jpg@4516-4842-1575-1984   1_D.jpg  4516  1575  4842  1984         0.529677"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['section_id', 'file_name', 'xmin', 'xmax', 'ymin', 'ymax', 'detection_score']\n",
    "df_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Loop over all detected boxes and convert the information into a dictionary\n",
    "for box in detections[0]:\n",
    "    xmin, ymin, xmax, ymax, score = box\n",
    "    xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)  # Convert predicted coordinates to integer values\n",
    "    box_dict = dict(\n",
    "        section_id=f'{example_image}@{xmin}-{xmax}-{ymin}-{ymax}',\n",
    "        file_name=example_image,\n",
    "        xmin=xmin,\n",
    "        ymin=ymin,\n",
    "        xmax=xmax,\n",
    "        ymax=ymax,\n",
    "        detection_score=score\n",
    "    )\n",
    "    predictions.append(box_dict)\n",
    "\n",
    "# Convert the dictionary into a dataframe with section_id as index\n",
    "prediction_df = pd.DataFrame(predictions)\n",
    "prediction_df.set_index('section_id', inplace=True)\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we restrict our dataset to only the file for which we made predictions (*1_D.jpg*) and run the score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = section_df.loc[section_df.file_name==example_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = PredictionEvaluator(ground_truth)\n",
    "thresholds = [0.5, 0.6, 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the leaderboard, the predicted worm section boxes are compared to the actual worm section boxes. We compute the [Intersection over Union (IOU)](https://towardsdatascience.com/iou-a-better-detection-evaluation-metric-45a511185be1) between the boxes. If the value is greater than a threshold (e.g. 0.5) the prediction counts as a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'detection_acc@iou0.5': 27.27,\n",
       " 'detection_tp@iou0.5': 30,\n",
       " 'detection_fp@iou0.5': 70,\n",
       " 'detection_fn@iou0.5': 10,\n",
       " 'detection_acc@iou0.6': 26.13,\n",
       " 'detection_tp@iou0.6': 29,\n",
       " 'detection_fp@iou0.6': 71,\n",
       " 'detection_fn@iou0.6': 11,\n",
       " 'detection_acc@iou0.7': 16.67,\n",
       " 'detection_tp@iou0.7': 20,\n",
       " 'detection_fp@iou0.7': 80,\n",
       " 'detection_fn@iou0.7': 20,\n",
       " 'score': 70.07}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_leaderboard_score(prediction_df, thresholds, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *get_leaderboard_score* gives us a lot of information that we can use to debug our model. \n",
    "- *score* is the score that will be shown in the leaderboard. It is the sum of the individual detection_acc@X values.\n",
    "- Additionally, for each IOU threshold the *accuracy, true positives, false positives* and *false negatives* are computed.\n",
    "\n",
    "**Exercise:**\n",
    "- Which type of error is the most common? How could you combat this?\n",
    "- The output shows true positives (tp), false positives (fp), false negatives (fn) but no true negatives. Why is that?\n",
    "- Evaluate the model on the *dummy_test* dataset we created previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first submission\n",
    "\n",
    "Now, we have all the pieces in place to create our first submission. We will predict the worm sections for all files and store them in a DataFrame.\n",
    "We will need to provide a prediction for all the files that are listed in *test_files.csv*. Let's load the file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['100_D.jpg', '100_C.jpg', '100_B.jpg', '100_AA.jpg', '100_A.jpg',\n",
       "       '101_DD.jpg', '101_C.jpg', '101_B.jpg', '101_AA.jpg', '101_A.jpg',\n",
       "       '86_D.jpg', '86_C.jpg', '86_B.jpg', '86_AA.jpg', '86_A.jpg',\n",
       "       '88_D.jpg', '88_C.jpg', '88_B.jpg', '88_A.jpg', '89_D.jpg',\n",
       "       '89_C.jpg', '89_B.jpg', '89_AA.jpg', '89_A.jpg', '90_D.jpg',\n",
       "       '90_C.jpg', '90_B.jpg', '90_AA.jpg', '90_A.jpg', '91_D.jpg',\n",
       "       '91_C.jpg', '91_B.jpg', '91_AA.jpg', '91_A.jpg', '92_D.jpg',\n",
       "       '92_C.jpg', '92_B.jpg', '92_AA.jpg', '92_A.jpg', '93_D.jpg',\n",
       "       '93_C.jpg', '93_B.jpg', '93_AA.jpg', '93_A.jpg', '94_D.jpg',\n",
       "       '94_C.jpg', '94_B.jpg', '94_AA.jpg', '94_A.jpg', '95_D.jpg',\n",
       "       '95_C.jpg', '95_B.jpg', '95_AA.jpg', '95_A.jpg', '96_D.jpg',\n",
       "       '96_C.jpg', '96_B.jpg', '96_AA.jpg', '96_A.jpg', '97_D.jpg',\n",
       "       '97_C.jpg', '97_B.jpg', '97_A.jpg', '98_D.jpg', '98_C.jpg',\n",
       "       '98_B.jpg', '98_AA.jpg', '98_A.jpg', '99_D.jpg', '99_C.jpg',\n",
       "       '99_B.jpg', '99_AA.jpg', '99_A.jpg'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = pd.read_csv(f'{data_folder}/test_files.csv', sep=';', header=None)\n",
    "file_names = files[0].values\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we created a function that runs the above code on all file names and returns a dataframe with the predictions. This will take around five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /root/data/AmazonSageMaker-gdsc-tutorials/data/tutorial_exps/epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [01:22<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "prediction_df = create_predictions(file_names, cfg, checkpoint, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>detection_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100_D.jpg@664-851-3168-3777</th>\n",
       "      <td>100_D.jpg</td>\n",
       "      <td>664</td>\n",
       "      <td>3168</td>\n",
       "      <td>851</td>\n",
       "      <td>3777</td>\n",
       "      <td>0.540133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100_D.jpg@707-904-919-1386</th>\n",
       "      <td>100_D.jpg</td>\n",
       "      <td>707</td>\n",
       "      <td>919</td>\n",
       "      <td>904</td>\n",
       "      <td>1386</td>\n",
       "      <td>0.532225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100_D.jpg@1259-1454-4218-4594</th>\n",
       "      <td>100_D.jpg</td>\n",
       "      <td>1259</td>\n",
       "      <td>4218</td>\n",
       "      <td>1454</td>\n",
       "      <td>4594</td>\n",
       "      <td>0.484710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100_D.jpg@1177-1748-2206-2716</th>\n",
       "      <td>100_D.jpg</td>\n",
       "      <td>1177</td>\n",
       "      <td>2206</td>\n",
       "      <td>1748</td>\n",
       "      <td>2716</td>\n",
       "      <td>0.472370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100_D.jpg@2788-2964-6293-6569</th>\n",
       "      <td>100_D.jpg</td>\n",
       "      <td>2788</td>\n",
       "      <td>6293</td>\n",
       "      <td>2964</td>\n",
       "      <td>6569</td>\n",
       "      <td>0.471982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               file_name  xmin  ymin  xmax  ymax  \\\n",
       "section_id                                                         \n",
       "100_D.jpg@664-851-3168-3777    100_D.jpg   664  3168   851  3777   \n",
       "100_D.jpg@707-904-919-1386     100_D.jpg   707   919   904  1386   \n",
       "100_D.jpg@1259-1454-4218-4594  100_D.jpg  1259  4218  1454  4594   \n",
       "100_D.jpg@1177-1748-2206-2716  100_D.jpg  1177  2206  1748  2716   \n",
       "100_D.jpg@2788-2964-6293-6569  100_D.jpg  2788  6293  2964  6569   \n",
       "\n",
       "                               detection_score  \n",
       "section_id                                      \n",
       "100_D.jpg@664-851-3168-3777           0.540133  \n",
       "100_D.jpg@707-904-919-1386            0.532225  \n",
       "100_D.jpg@1259-1454-4218-4594         0.484710  \n",
       "100_D.jpg@1177-1748-2206-2716         0.472370  \n",
       "100_D.jpg@2788-2964-6293-6569         0.471982  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! All that's left is to save the results to a csv and upload it on the [GDSC website](https://gdsc.ce.capgemini.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(f'{data_folder}/results_tutorial3.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission should lead to a score of around 28. \n",
    "\n",
    "**Exercise:**\n",
    "- The submission score is a lot less than the score of 70.07 we computed above. What are potential reasons for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook covered a LOT of different topics. We covered how to \n",
    "- Train a basic object detection model\n",
    "- Use a trained model to create predictions on a nodule image\n",
    "- How to evaluate the predictions\n",
    "- How to create a first submission\n",
    "\n",
    "all done on a dummy dataset. With this, we have the technical foundations to creating a good submission!\n",
    "\n",
    "In the next tutorial, you will learn how to run and evaluate the model from this notebook on the complete dataset as a Sagemaker training job.\n",
    "\n",
    "**REMINDER: Remember to shut down the *ml.g4dn.xlarge* instance when you aren't using it.**"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (gdsc5-smstudio-custom/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:954362353459:image-version/gdsc5-smstudio-custom/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
