{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945efe73-849c-47ca-8c0b-878318f672a4",
   "metadata": {},
   "source": [
    "# ENet Train\n",
    "\n",
    "This code ist based on the Enet impementation of following GitHub repository: https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation <br/>\n",
    "Link to the Enet paper: https://arxiv.org/pdf/1606.02147.pdf <br/>\n",
    "K-Fold cross validation is used as evaluation method <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a4444-de62-41d6-814f-03a5d850ae88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81aecf7-8dd7-4fda-bf40-75d4eff22785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "\n",
    "from enetModules.ENet import ENet\n",
    "from enetModules.Utils import Utils\n",
    "#from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchmetrics import Dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e10d93-121a-4025-9593-98e34db36a4a",
   "metadata": {},
   "source": [
    "# Default Settings\n",
    "**Change paths here if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a872e25d-c6b2-411f-af1d-07ae776d6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "batch_size = 32\n",
    "epochs_per_fold = 1 # 80\n",
    "learn_rate = 5e-4 \n",
    "n_classes = 2\n",
    "weight_decay = 2e-4\n",
    "k_folds = 2 # 5\n",
    "test_val_split_size = 0.15\n",
    "#description = \"Diese Evaluation wurde mit den mit vorverarbeiteten Schistosoma Mansoni Datensatz getestet. Preprocessing: adjustProperties => brightness=1.1, contrast=3.1, sharpness=2.1, saturation=2.1, grayscale=True. Logik für low visibility wird angewendet\"\n",
    "description = \"Diese Evaluation wurde mit den Schistosoma Mansoni Datensatz getestet. Datenaugmentierung wurde angewendet.\"\n",
    "\n",
    "path_save_model = \"./modelEvaluation/\"\n",
    "#path_images = \"../../content/SchistosomaMansoni/img/\"\n",
    "#path_labels = \"../../content/SchistosomaMansoni/labels/\"\n",
    "#path_images_val = \"../../content/SchistosomaMansoni/val_img/\"\n",
    "#path_labels_val = \"../..//content/SchistosomaMansoni/val_labels/\"\n",
    "path_images = \"../../content/RiverBlindness/img/\"\n",
    "path_labels = \"../../content/RiverBlindness/labels/\"\n",
    "path_images_val = \"../../content/RiverBlindness/val_img/\"\n",
    "path_labels_val = \"../..//content/RiverBlindness/val_labels/\"\n",
    "\n",
    "with open(path_save_model + \"logs/settings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    settings = { \n",
    "        \"cuda\": cuda,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs_per_fold\": epochs_per_fold,\n",
    "        \"learn_rate\": learn_rate,\n",
    "        \"n_classes\": n_classes,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"k_folds\": k_folds,\n",
    "        \"test_val_split_size\": test_val_split_size,\n",
    "        \"description\": description\n",
    "    }\n",
    "    \n",
    "    json.dump(settings, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55195b-6234-44d3-8f7c-8a994354ec26",
   "metadata": {},
   "source": [
    "# Evaluation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfeaebf1-e364-4e1d-a601-95bcd9f7810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Starting to define the class weights...\n",
      "[INFO]Fetched all class weights successfully!\n",
      "[INFO]CUDA isn't available!\n",
      "[INFO]Defined the loss function\n"
     ]
    }
   ],
   "source": [
    "img_filenames = np.array(os.listdir(path_images))\n",
    "input_train = []\n",
    "\n",
    "label_filenames = np.array(os.listdir(path_labels))\n",
    "label_train = []\n",
    "\n",
    "assert(len(img_filenames) == len(label_filenames))\n",
    "\n",
    "# Reading train images and labels                  \n",
    "for file in img_filenames:    \n",
    "    img = Image.open(path_images + file)\n",
    "    augmented_imgs = Utils.imageAugmentation(img)\n",
    "        \n",
    "    for aug_img in augmented_imgs:\n",
    "        img = cv2.resize(np.array(aug_img), (512, 512), cv2.INTER_NEAREST)\n",
    "        input_train.append(img[:,:,0:3]) # cutting out potential alpha channel\n",
    "    \n",
    "    label = Image.open(path_labels + file)\n",
    "    augmented_labels = Utils.imageAugmentation(label)\n",
    "     \n",
    "    for aug_label in augmented_labels:\n",
    "        label = cv2.resize(np.array(aug_label), (512, 512), cv2.INTER_NEAREST)\n",
    "        label_train.append(label)\n",
    "\n",
    "input_train = np.stack(input_train, axis=2)\n",
    "input_train = torch.tensor(input_train).transpose(0, 2).transpose(1, 3)\n",
    "\n",
    "label_train = np.array(label_train) \n",
    "label_train = torch.tensor(label_train)\n",
    "\n",
    "# Reading validation images and labels\n",
    "val_img_filenames = np.array(os.listdir(path_images_val))\n",
    "input_val = []\n",
    "\n",
    "val_label_filenames = np.array(os.listdir(path_labels_val))\n",
    "label_val = []\n",
    "\n",
    "assert(len(val_img_filenames) == len(val_label_filenames))\n",
    "\n",
    "# Validation set should not be augmented\n",
    "for file in val_img_filenames:\n",
    "    val_img = Image.open(path_images_val + file)\n",
    "    val_img = cv2.resize(np.array(val_img), (512, 512), cv2.INTER_NEAREST)\n",
    "    input_val.append(val_img[:,:,0:3])\n",
    "    \n",
    "    val_label = Image.open(path_labels_val + file)\n",
    "    val_label = cv2.resize(np.array(val_label), (512, 512), cv2.INTER_NEAREST)\n",
    "    label_val.append(val_label)\n",
    "    \n",
    "input_val = np.stack(input_val, axis=2)\n",
    "input_val = torch.tensor(input_val).transpose(0,2).transpose(1,3)\n",
    "\n",
    "label_val = np.array(label_val)\n",
    "label_val = torch.tensor(label_val)\n",
    "\n",
    "print(\"[INFO]Starting to define the class weights...\")\n",
    "class_weights = Utils.get_class_weights(label_train, n_classes)\n",
    "print(\"[INFO]Fetched all class weights successfully!\")\n",
    "\n",
    "# Checking for cuda\n",
    "if(torch.cuda.is_available() & cuda):\n",
    "    print(\"[INFO]CUDA is available!\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"[INFO]CUDA isn't available!\")\n",
    "    device = torch.device(\"cpu\")\n",
    "                \n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
    "print(\"[INFO]Defined the loss function\")                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fb0f5b-6e80-47e0-a02b-2d1e6d47f545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Starting Training...\n",
      "===========================\n",
      "         Fold: 1\n",
      "===========================\n",
      "[INFO]Model and optimizer instantiated!\n",
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [03:07<00:00, 14.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1... Loss 11.673788\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1.624633\n",
      "Epoch 1/1...\n",
      "[INFO]Starting Evaluation for Fold 1.\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Jaccard Index: \t meanClass[0] = 0.4001237552754487 \t meanClass[1] = 0.3659361651049385 \t mean = 0.3830299601901936\n",
      "Dice Score: \t meanClass[0] = 0.5656030826176269 \t meanClass[1] = 0.5333628910251811 \t mean = 0.549482986821404\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "[INFO]Saving metric data and model.\n",
      "[INFO]Saving done.\n",
      "===========================\n",
      "         Fold: 2\n",
      "===========================\n",
      "[INFO]Model and optimizer instantiated!\n",
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [02:04<01:46, 17.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4fa62852286d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viktor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\viktor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"[INFO]Starting Training...\")\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=False)\n",
    "\n",
    "# Important values and metrics for backup\n",
    "train_losses_json = {}\n",
    "val_losses_json = {}\n",
    "jaccard_index_per_fold_json = []\n",
    "jaccard_index_means_json = []\n",
    "dice_score_per_fold_json = []\n",
    "dice_score_means_json = []\n",
    "\n",
    "for k, (kfsplit_train, kfsplit_test) in enumerate(kf.split(input_train)): \n",
    "    print(\"===========================\")\n",
    "    print(f\"         Fold: {k + 1}\")     \n",
    "    print(\"===========================\")\n",
    "   \n",
    "    enet = ENet(n_classes)\n",
    "    enet = enet.to(device)\n",
    "    optimizer = torch.optim.Adam(enet.parameters(),\n",
    "                                lr=learn_rate,\n",
    "                                weight_decay=weight_decay)\n",
    "    print(\"[INFO]Model and optimizer instantiated!\")\n",
    "    \n",
    "    kf_X_train, kf_X_test = input_train[kfsplit_train], input_train[kfsplit_test]\n",
    "    kf_y_train, kf_y_test = label_train[kfsplit_train], label_train[kfsplit_test]\n",
    "         \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    batch_count_train = (len(kf_X_train) // batch_size)\n",
    "    batch_count_val = (len(input_val) // batch_size)\n",
    "      \n",
    "    if((len(kf_X_train) / batch_size) % 1 != 0):\n",
    "        batch_count_train += 1\n",
    "        \n",
    "    if((len(input_val) / batch_size) % 1 != 0):\n",
    "        batch_count_val += 1\n",
    "        \n",
    "    for e in range(1, epochs_per_fold + 1):\n",
    "        train_loss = 0\n",
    "        print (\"-\"*15,\"Epoch %d\" % e , \"-\"*15) \n",
    "\n",
    "        enet.train()\n",
    "             \n",
    "        for _ in tqdm(range(batch_count_train)):           \n",
    "            X_train, y_train = kf_X_train[batch_size * _: batch_size * (_ + 1)], kf_y_train[batch_size * _: batch_size * (_ + 1)]          \n",
    "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "                                     \n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "            out = enet(X_train.float())        \n",
    "            loss = criterion(out, y_train.long())           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                       \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        print()\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    " \n",
    "        print ('Epoch {}/{}...'.format(e, epochs_per_fold),\n",
    "                'Loss {:6f}'.format(train_loss))\n",
    "            \n",
    "        with torch.no_grad():                \n",
    "            print()\n",
    "            print(\"Validation:\")\n",
    "\n",
    "            # Validates the model\n",
    "            enet.eval()              \n",
    "            val_loss = 0\n",
    "\n",
    "            for _ in tqdm(range(batch_count_val)):           \n",
    "                X_val, y_val = input_val[batch_size * _: batch_size * (_ + 1)], label_val[batch_size * _: batch_size * (_ + 1)]          \n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "                out = enet(X_val.float())\n",
    "                loss = criterion(out, y_val.long())\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            print('Loss {:6f}'.format(val_loss))\n",
    "\n",
    "            val_losses.append(val_loss)            \n",
    "                                                                       \n",
    "        print ('Epoch {}/{}...'.format(e, epochs_per_fold))\n",
    "       \n",
    "    print(f\"[INFO]Starting Evaluation for Fold {k+1}.\")\n",
    "    \n",
    "    jaccard = JaccardIndex(num_classes=n_classes, average=\"none\")\n",
    "    dice = Dice(num_classes=n_classes, average=\"none\")\n",
    "    \n",
    "    jaccard_indices = []\n",
    "    dice_scores = []\n",
    "          \n",
    "    with torch.no_grad():\n",
    "        for test_sample_index in range(0, len(kf_X_test)):            \n",
    "            out_test = enet(kf_X_test[test_sample_index].unsqueeze(0).float()).squeeze(0)\n",
    "            pred = out_test.data.max(0)[1].cpu().numpy()\n",
    "            pred = torch.tensor(pred.astype('uint8'))\n",
    "                                           \n",
    "            jaccard_indices.append(jaccard(pred, kf_y_test[test_sample_index]).tolist())\n",
    "            dice_scores.append(dice(pred, kf_y_test[test_sample_index]).tolist())\n",
    "   \n",
    "    jaccard_index_means_per_class = [np.mean([jindex[0] for jindex in jaccard_indices]), np.mean([jindex[1] for jindex in jaccard_indices])]\n",
    "    dice_score_means_per_class = [np.mean([dscore[0] for dscore in dice_scores]), np.mean([dscore[1] for dscore in dice_scores])]\n",
    "                            \n",
    "    jaccard_index_per_fold_json.append(jaccard_indices)\n",
    "    dice_score_per_fold_json.append(dice_scores)\n",
    "    \n",
    "    jaccard_index_means_json.append(jaccard_index_means_per_class)\n",
    "    dice_score_means_json.append(dice_score_means_per_class)   \n",
    "\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------------------------\")      \n",
    "    print(f\"Jaccard Index: \\t meanClass[0] = {str(jaccard_index_means_per_class[0])} \\t meanClass[1] = {str(jaccard_index_means_per_class[1])} \\t mean = {np.mean(jaccard_index_means_per_class)}\")                                  \n",
    "    print(f\"Dice Score: \\t meanClass[0] = {str(dice_score_means_per_class[0])} \\t meanClass[1] = {str(dice_score_means_per_class[1])} \\t mean = {np.mean(dice_score_means_per_class)}\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------------------------\")   \n",
    "        \n",
    "    train_losses_json[f\"fold{k+1}\"] = train_losses\n",
    "    val_losses_json[f\"fold{k+1}\"] = val_losses\n",
    "    \n",
    "    print(\"[INFO]Saving metric data and model.\")\n",
    "       \n",
    "    with open(path_save_model + \"logs/trainLosses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_losses_json, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    with open(path_save_model + \"logs/valLosses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(val_losses_json, f, ensure_ascii=False, indent=4)    \n",
    "        \n",
    "    with open(path_save_model + \"logs/jaccardIndicesPerFold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(jaccard_index_per_fold_json, f, ensure_ascii=False, indent=4) \n",
    "        \n",
    "    with open(path_save_model + \"logs/diceScoresPerFold.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dice_score_per_fold_json, f, ensure_ascii=False, indent=4) \n",
    "            \n",
    "    with open(path_save_model + \"logs/jaccardIndexMeans.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(jaccard_index_means_json, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    with open(path_save_model + \"logs/diceScoreMeans.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dice_score_means_json, f, ensure_ascii=False, indent=4) \n",
    "        \n",
    "    checkpoint = {\n",
    "        'fold': k+1,\n",
    "        'state_dict' : enet.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path_save_model + 'ckpt-enet-{}-{}.pth'.format(k+1, train_loss))\n",
    "\n",
    "    print(\"[INFO]Saving done.\")\n",
    "            \n",
    "print(\"[INFO]Evaluation process complete!\")                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c25f47c-4aae-4813-b2ce-d821b027a7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
