{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f076284a-a792-4e36-95f7-28d1a12d5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "\n",
    "from enetModules.ENet import ENet\n",
    "from enetModules.ENet import RDDNeck\n",
    "from enetModules.ENet import UBNeck\n",
    "from enetModules.ENet import ASNeck\n",
    "from enetModules.Utils import Utils\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe447e58-b433-4b4d-a946-ec6d302aebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "learn_rate = 5e-4 \n",
    "save_every = 5 \n",
    "n_classes = 2\n",
    "weight_decay = 2e-4\n",
    "test_val_split_size = 0.10\n",
    "description = \"Diese System wurde mit den unbehandelten River Blindness Datensatz trainiert.\"\n",
    "\n",
    "path_load_model = \"./modelBackups/Model_29_11_22_best_SM/ckpt-enet-80-0.19326677545905113.pth\"\n",
    "path_save_model = \"./transferTrainModel/\"\n",
    "path_images = \"./content/RiverBlindness/img/\"\n",
    "path_labels = \"./content/RiverBlindness/labels/\"\n",
    "\n",
    "with open(path_save_model + \"logs/settings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    settings = { \n",
    "        \"cuda\": cuda,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"learn_rate\": learn_rate,\n",
    "        \"n_classes\": n_classes,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"test_val_split_size\": test_val_split_size,\n",
    "        \"description\": description\n",
    "    }\n",
    "    \n",
    "    json.dump(settings, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d6ddde-932a-4720-9db9-7ea34e5b926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freezeLayer(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreezeLayer(layer):\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209128b3-c3f8-4178-8574-0d2386eac60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Starting to define the class weights...\n",
      "[INFO]Fetched all class weights successfully!\n",
      "[INFO]CUDA isn't available!\n",
      "[INFO]Training model form path ====> ./modelBackups/Model_29_11_22_best_SM/ckpt-enet-80-0.19326677545905113.pth\n",
      "[INFO]Model Loaded!\n",
      "[INFO]Freezing Layer.\n",
      "[INFO]Model Instantiated!\n",
      "[INFO]Defined the loss function and the optimizer\n"
     ]
    }
   ],
   "source": [
    "img_filenames = np.array(os.listdir(path_images))\n",
    "inputs = []\n",
    "\n",
    "label_filenames = np.array(os.listdir(path_labels))\n",
    "labels = []\n",
    "\n",
    "assert(len(img_filenames) == len(label_filenames))\n",
    "\n",
    "# Reading images and labels                  \n",
    "for file in img_filenames:    \n",
    "    img = Image.open(path_images + file)\n",
    "    augmented_imgs = Utils.imageAugmentation(img)\n",
    "    \n",
    "    for aug_img in augmented_imgs:\n",
    "        img = cv2.resize(np.array(aug_img), (512, 512), cv2.INTER_NEAREST)\n",
    "        inputs.append(img[:,:,0:3]) # cutting out potential alpha channel\n",
    "    \n",
    "    label = Image.open(path_labels + file)\n",
    "    augmented_labels = Utils.imageAugmentation(label)\n",
    "    \n",
    "    for aug_label in augmented_labels:\n",
    "        label = cv2.resize(np.array(aug_label), (512, 512), cv2.INTER_NEAREST)\n",
    "        labels.append(label)\n",
    "    \n",
    "inputs = np.stack(inputs, axis=2)\n",
    "inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n",
    "\n",
    "labels = np.array(labels) \n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "#Shuffles the datasets\n",
    "input_train, input_val, label_train, label_val = train_test_split(inputs, labels, test_size=test_val_split_size)\n",
    "\n",
    "batch_count_train = (len(input_train) // batch_size)\n",
    "batch_count_val = (len(input_val) // batch_size)\n",
    "\n",
    "if((len(input_train) / batch_size) % 1 != 0):\n",
    "    batch_count_train += 1\n",
    "\n",
    "if((len(input_val) / batch_size) % 1 != 0):\n",
    "    batch_count_val += 1\n",
    "\n",
    "print(\"[INFO]Starting to define the class weights...\")\n",
    "class_weights = Utils.get_class_weights(labels, n_classes)\n",
    "print(\"[INFO]Fetched all class weights successfully!\")\n",
    "\n",
    "# Checking for cuda\n",
    "if(torch.cuda.is_available() & cuda):\n",
    "    print(\"[INFO]CUDA is available!\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"[INFO]CUDA isn't available!\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"[INFO]Training model form path ====> \" + path_load_model)\n",
    "    \n",
    "preTrainedModel = torch.load(path_load_model, map_location=device)\n",
    "\n",
    "enet = ENet(n_classes)\n",
    "enet.load_state_dict(preTrainedModel['state_dict'])\n",
    "enet = enet.to(device)\n",
    "\n",
    "print (\"[INFO]Model Loaded!\")\n",
    "print (\"[INFO]Freezing Layer.\")\n",
    "\n",
    "freezeLayer(enet.init)\n",
    "freezeLayer(enet.b10)\n",
    "freezeLayer(enet.b11)  \n",
    "freezeLayer(enet.b12)\n",
    "freezeLayer(enet.b13)  \n",
    "freezeLayer(enet.b14)\n",
    "\n",
    "freezeLayer(enet.b20)  \n",
    "freezeLayer(enet.b21)\n",
    "freezeLayer(enet.b22)  \n",
    "freezeLayer(enet.b23)\n",
    "freezeLayer(enet.b24)  \n",
    "freezeLayer(enet.b25)\n",
    "freezeLayer(enet.b26)  \n",
    "freezeLayer(enet.b27)\n",
    "freezeLayer(enet.b28) \n",
    "\n",
    "freezeLayer(enet.b31)\n",
    "freezeLayer(enet.b32)  \n",
    "freezeLayer(enet.b33)\n",
    "freezeLayer(enet.b34)  \n",
    "freezeLayer(enet.b35)\n",
    "freezeLayer(enet.b36)  \n",
    "freezeLayer(enet.b37)\n",
    "freezeLayer(enet.b38) \n",
    "    \n",
    "freezeLayer(enet.b40)\n",
    "freezeLayer(enet.b41)      \n",
    "freezeLayer(enet.b42)\n",
    "\n",
    "freezeLayer(enet.b50)    \n",
    "freezeLayer(enet.b51)\n",
    "\n",
    "# Set a new output layer\n",
    "'''\n",
    "enet.b34 = RDDNeck(dilation=4, \n",
    "                    in_channels=128, \n",
    "                    out_channels=128, \n",
    "                    down_flag=False)\n",
    "\n",
    "enet.b35 =  RDDNeck(dilation=1, \n",
    "                    in_channels=128, \n",
    "                    out_channels=128, \n",
    "                    down_flag=False)\n",
    "\n",
    "enet.b36 = RDDNeck(dilation=8, \n",
    "                    in_channels=128, \n",
    "                    out_channels=128, \n",
    "                    down_flag=False)\n",
    "\n",
    "enet.b37 = ASNeck(in_channels=128, \n",
    "                    out_channels=128)\n",
    "\n",
    "enet.b38 = RDDNeck(dilation=16, \n",
    "                    in_channels=128, \n",
    "                    out_channels=128, \n",
    "                    down_flag=False)\n",
    "\n",
    "\n",
    "enet.b40 = UBNeck(in_channels=128, \n",
    "                    out_channels=64, \n",
    "                    relu=True)\n",
    "\n",
    "enet.b41 = RDDNeck(dilation=1, \n",
    "                    in_channels=64, \n",
    "                    out_channels=64, \n",
    "                    down_flag=False, \n",
    "                    relu=True)\n",
    "\n",
    "enet.b42 = RDDNeck(dilation=1, \n",
    "                    in_channels=64, \n",
    "                    out_channels=64, \n",
    "                    down_flag=False, \n",
    "                    relu=True)\n",
    "\n",
    "\n",
    "enet.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "\n",
    "enet.b51 = RDDNeck(dilation=1, \n",
    "                    in_channels=16, \n",
    "                    out_channels=16, \n",
    "                    down_flag=False, \n",
    "                    relu=True)\n",
    "'''\n",
    "\n",
    "enet.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                    out_channels= n_classes, \n",
    "                                    kernel_size=3, \n",
    "                                    stride=2, \n",
    "                                    padding=1, \n",
    "                                    output_padding=1,\n",
    "                                    bias=False)\n",
    "    \n",
    "print (\"[INFO]Model Instantiated!\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
    "optimizer = torch.optim.Adam(enet.parameters(),\n",
    "                                lr=learn_rate,\n",
    "                                weight_decay=weight_decay)\n",
    "print(\"[INFO]Defined the loss function and the optimizer\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f04197b-032b-49a4-b386-aeb5ae06296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Staring Training...\n",
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:36<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1... Loss 9.692383\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1.485765\n",
      "Epoch 1/1...\n",
      "[INFO]Training Process complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"[INFO]Staring Training...\")\\n\\nkf = KFold(n_splits=k_folds, shuffle=False)\\n\\n# important values and metrics for backup\\ntrain_losses_json = {}\\nval_losses_json = {}\\njaccard_index_per_fold_json = []\\njaccard_index_means_json = []\\ndice_score_per_fold_json = []\\ndice_score_means_json = []\\n\\n\\nfor k, (kfsplit_train, kfsplit_test) in enumerate(kf.split(input_train)): \\n    print(\"===========================\")\\n    print(f\"         Fold: {k + 1}\")     \\n    print(\"===========================\")\\n        \\n    X_train, X_test = input_train[kfsplit_train], input_train[kfsplit_test]\\n    y_train, y_test = label_train[kfsplit_train], label_train[kfsplit_test]\\n    \\n    train_losses = []\\n    val_losses = []\\n    \\n    for e in range(1, epochs + 1):\\n        train_loss = 0\\n        print (\"-\"*15,\"Epoch %d\" % e , \"-\"*15) \\n\\n        enet.train()\\n\\n        for _ in tqdm(range(batch_count_train)):       \\n            X_train, y_train = input_train[batch_size * _: batch_size * (_ + 1)], label_train[batch_size * _: batch_size * (_ + 1)]        \\n            X_train, y_train = X_train.to(device), y_train.to(device)\\n               \\n            optimizer.zero_grad()\\n          \\n            out = enet(X_train.float())        \\n            loss = criterion(out, y_train.long())           \\n            loss.backward()\\n            optimizer.step()\\n                       \\n            train_loss += loss.item()\\n\\n        print()\\n        train_losses.append(train_loss)\\n\\n        if e % print_every == 0:\\n            print (\\'Epoch {}/{}...\\'.format(e, epochs),\\n                    \\'Loss {:6f}\\'.format(train_loss))\\n        \\n        if e % val_every == 0:\\n            with torch.no_grad():                \\n                print()\\n                print(\"Validation:\")\\n                \\n                # Validates the model\\n                enet.eval()              \\n                val_loss = 0\\n                               \\n                for _ in tqdm(range(batch_count_val)):           \\n                    X_val, y_val = input_val[val_every * _: val_every * (_ + 1)], label_val[val_every * _: val_every * (_ + 1)]\\n                    X_val, y_val = X_val.to(device), y_val.to(device)\\n                                        \\n                    out = enet(X_val)\\n                    loss = criterion(out, y_val.long())\\n                    \\n                    val_loss += loss.item()\\n                    \\n                print(\\'Loss {:6f}\\'.format(val_loss))\\n                \\n                val_losses.append(val_loss)            \\n\\n        if e % save_every == 0:\\n            checkpoint = {\\n                \\'fold\\': k+1,\\n                \\'epochs\\' : e,\\n                \\'state_dict\\' : enet.state_dict()\\n            }\\n                    \\n            torch.save(checkpoint, path_save_model + \\'ckpt-enet-{}-{}-{}.pth\\'.format(k+1, e, train_loss))\\n            print()\\n            print(\"Model saved!\")       \\n                          \\n        print (\\'Epoch {}/{}...\\'.format(e, epochs))\\n        \\n    print(f\"[INFO]Starting Evaluation for Fold {k+1}.\")\\n     \\n    jaccard = JaccardIndex(num_classes=n_classes, average=\"macro\")\\n    dice = Dice(num_classes=n_classes, average=\"macro\")\\n    \\n    jaccard_indices = []\\n    dice_scores = []\\n    \\n    with torch.no_grad():\\n        for test_sample_index in range(0, len(X_test)):            \\n            out_test = enet(X_test[test_sample_index].unsqueeze(0)).float().squeeze(0)          \\n            pred = out_test.data.max(0)[1].cpu()\\n           \\n            jaccard_indices.append(jaccard(pred, y_test[test_sample_index].unsqueeze(0)).item())\\n            dice_scores.append(dice(pred, y_test[test_sample_index].unsqueeze(0)).item())\\n   \\n    jaccard_index_mean = np.array(jaccard_indices).mean()\\n    dice_score_mean = np.array(dice_scores).mean()\\n                         \\n    jaccard_index_per_fold_json.append(jaccard_indices)\\n    dice_score_per_fold_json.append(dice_scores)\\n    \\n    jaccard_index_means_json.append(jaccard_index_mean)\\n    dice_score_means_json.append(dice_score_mean)     \\n    \\n    print()      \\n    print(\"Mean Jaccard Index: \" + str(jaccard_index_mean))                                  \\n    print(\"Mean Dice Score: \" + str(dice_score_mean))\\n    print()\\n    \\n    train_losses_json[f\"fold{k+1}\"] = train_losses\\n    val_losses_json[f\"fold{k+1}\"] = val_losses\\n    \\n    print(\"[INFO]Saving metric data.\")\\n       \\n    with open(path_save_model + \"logs/trainLosses.json\", \"w\", encoding=\"utf-8\") as f:\\n        json.dump(train_losses_json, f, ensure_ascii=False, indent=4)\\n        \\n    with open(path_save_model + \"logs/valLosses.json\", \"w\", encoding=\"utf-8\") as f:\\n        json.dump(val_losses_json, f, ensure_ascii=False, indent=4)    \\n        \\n    with open(path_save_model + \"logs/jaccardIndicesPerFold.json\", \"w\", encoding=\"utf-8\") as f:\\n        json.dump(jaccard_index_per_fold_json, f, ensure_ascii=False, indent=4) \\n        \\n    with open(path_save_model + \"logs/diceScoresPerFold.json\", \"w\", encoding=\"utf-8\") as f:\\n        json.dump(dice_score_per_fold_json, f, ensure_ascii=False, indent=4) \\n            \\n    with open(path_save_model + \"logs/jaccardIndexMeans.json\", \"w\", encoding=\"utf-8\") as f:\\n        json.dump(jaccard_index_means_json, f, ensure_ascii=False, indent=4)\\n    \\n    with open(path_save_model + \"logs/diceScoreMeans.json\", \"w\", encoding=\"utf-8\") as f:\\n        json.dump(dice_score_means_json, f, ensure_ascii=False, indent=4) \\n        \\n    print(\"[INFO]Saving done.\")    \\n\\nprint (\"[INFO]Training Process complete!\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[INFO]Staring Training...\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = 0\n",
    "    print (\"-\"*15,\"Epoch %d\" % e , \"-\"*15) \n",
    "\n",
    "    enet.train()\n",
    "\n",
    "    for _ in tqdm(range(batch_count_train)):                \n",
    "        X_train, y_train = input_train[batch_size * _: batch_size * (_ + 1)], label_train[batch_size * _: batch_size * (_ + 1)]        \n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = enet(X_train.float())        \n",
    "        loss = criterion(out, y_train.long())           \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print()\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print ('Epoch {}/{}...'.format(e, epochs),\n",
    "            'Loss {:6f}'.format(train_loss))\n",
    "\n",
    "    with torch.no_grad():                \n",
    "        print()\n",
    "        print(\"Validation:\")\n",
    "\n",
    "        # Validates the model\n",
    "        enet.eval()              \n",
    "        val_loss = 0\n",
    "\n",
    "        for _ in tqdm(range(batch_count_val)):           \n",
    "            X_val, y_val = input_val[batch_size * _: batch_size * (_ + 1)], label_val[batch_size * _: batch_size * (_ + 1)]\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "            out = enet(X_val.float())\n",
    "            loss = criterion(out, y_val.long())\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        print('Loss {:6f}'.format(val_loss))\n",
    "\n",
    "        val_losses.append(val_loss)            \n",
    "\n",
    "    if e % save_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, path_save_model + 'ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        \n",
    "        with open(path_save_model + \"logs/trainLosses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(train_losses, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        with open(path_save_model + \"logs/valLosses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(val_losses, f, ensure_ascii=False, indent=4) \n",
    "            \n",
    "        print()\n",
    "        print('Model and Losses saved!')\n",
    "\n",
    "    print ('Epoch {}/{}...'.format(e, epochs))         \n",
    "print(\"[INFO]Training Process complete!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
